{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Daily Time Series with Splits and Dividend Events with stock = RELIANCE.BSE was last refreshed at 2023-06-22 with timezone = US/Eastern\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "def get_ticker(stri):\n",
    "    url = f'https://www.alphavantage.co/query?function=SYMBOL_SEARCH&outputsize=full&keywords={stri}&apikey=DJ5ATAVVQLJE2IFG'\n",
    "    r = requests.get(url)\n",
    "    data = r.json()\n",
    "    return str(pd.DataFrame(data[\"bestMatches\"]).iloc[0][0])\n",
    "\n",
    "def get_data(ticker=\"TIME_SERIES_DAILY_ADJUSTED\",sym=\"IBM\"):\n",
    "    text=\"TIME_SERIES_INTRADAY\"\n",
    "    url = f\"https://www.alphavantage.co/query?function={ticker}&symbol={sym}&outputsize=full&interval=5min&apikey=DJ5ATAVVQLJE2IFG\"\n",
    "    r = requests.get(url)\n",
    "    data = r.json()\n",
    "    return data\n",
    "\n",
    "def clean_col_names(df):\n",
    "    for i in df.columns:\n",
    "        df.rename(columns={i:i.split(sep=\" \")[1].upper()}, inplace=True)\n",
    "        df.index = pd.to_datetime(df.index)\n",
    "        for i in df.columns:\n",
    "            df[i]=pd.to_numeric(df[i])\n",
    "    return df\n",
    "\n",
    "def get_details(data):\n",
    "    print(f'The {data[\"Meta Data\"].get(\"1. Information\")} with stock = {data[\"Meta Data\"].get(\"2. Symbol\")} was last refreshed at {data[\"Meta Data\"].get(\"3. Last Refreshed\")} with timezone = {data[\"Meta Data\"].get(\"5. Time Zone\")}')\n",
    "\n",
    "def stock(data):\n",
    "    return data[\"Meta Data\"].get(\"2. Symbol\")\n",
    "\n",
    "data=get_data(ticker=\"TIME_SERIES_DAILY_ADJUSTED\",sym=get_ticker(input()))\n",
    "to_get=list(data)[1]\n",
    "\n",
    "df = pd.DataFrame(data[to_get]).T\n",
    "stock = stock(data)\n",
    "clean_col_names(df)\n",
    "get_details(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OPEN</th>\n",
       "      <th>HIGH</th>\n",
       "      <th>LOW</th>\n",
       "      <th>CLOSE</th>\n",
       "      <th>ADJUSTED</th>\n",
       "      <th>VOLUME</th>\n",
       "      <th>DIVIDEND</th>\n",
       "      <th>SPLIT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2005-01-03</th>\n",
       "      <td>388.7685</td>\n",
       "      <td>407.0464</td>\n",
       "      <td>388.7685</td>\n",
       "      <td>405.9997</td>\n",
       "      <td>87.2292</td>\n",
       "      <td>11842921</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-04</th>\n",
       "      <td>399.9444</td>\n",
       "      <td>407.3827</td>\n",
       "      <td>395.8329</td>\n",
       "      <td>396.8048</td>\n",
       "      <td>85.2536</td>\n",
       "      <td>10059943</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-05</th>\n",
       "      <td>392.8427</td>\n",
       "      <td>399.1970</td>\n",
       "      <td>385.9277</td>\n",
       "      <td>396.1319</td>\n",
       "      <td>85.1091</td>\n",
       "      <td>16954266</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-06</th>\n",
       "      <td>392.4690</td>\n",
       "      <td>401.0658</td>\n",
       "      <td>390.6747</td>\n",
       "      <td>393.1791</td>\n",
       "      <td>84.4746</td>\n",
       "      <td>13446517</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2005-01-07</th>\n",
       "      <td>394.7115</td>\n",
       "      <td>409.2889</td>\n",
       "      <td>394.3751</td>\n",
       "      <td>404.3176</td>\n",
       "      <td>86.8678</td>\n",
       "      <td>16969845</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-16</th>\n",
       "      <td>2555.6001</td>\n",
       "      <td>2581.0000</td>\n",
       "      <td>2555.6001</td>\n",
       "      <td>2575.3999</td>\n",
       "      <td>2575.3999</td>\n",
       "      <td>150565</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-19</th>\n",
       "      <td>2582.2000</td>\n",
       "      <td>2584.0000</td>\n",
       "      <td>2543.3999</td>\n",
       "      <td>2551.5000</td>\n",
       "      <td>2551.5000</td>\n",
       "      <td>165566</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-20</th>\n",
       "      <td>2550.5000</td>\n",
       "      <td>2564.0000</td>\n",
       "      <td>2535.0000</td>\n",
       "      <td>2546.7500</td>\n",
       "      <td>2546.7500</td>\n",
       "      <td>171359</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-21</th>\n",
       "      <td>2556.3501</td>\n",
       "      <td>2569.1001</td>\n",
       "      <td>2547.9500</td>\n",
       "      <td>2563.6001</td>\n",
       "      <td>2563.6001</td>\n",
       "      <td>144710</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2023-06-22</th>\n",
       "      <td>2561.7000</td>\n",
       "      <td>2572.8999</td>\n",
       "      <td>2532.2500</td>\n",
       "      <td>2535.6001</td>\n",
       "      <td>2535.6001</td>\n",
       "      <td>61620</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4553 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 OPEN       HIGH        LOW      CLOSE   ADJUSTED    VOLUME  \\\n",
       "2005-01-03   388.7685   407.0464   388.7685   405.9997    87.2292  11842921   \n",
       "2005-01-04   399.9444   407.3827   395.8329   396.8048    85.2536  10059943   \n",
       "2005-01-05   392.8427   399.1970   385.9277   396.1319    85.1091  16954266   \n",
       "2005-01-06   392.4690   401.0658   390.6747   393.1791    84.4746  13446517   \n",
       "2005-01-07   394.7115   409.2889   394.3751   404.3176    86.8678  16969845   \n",
       "...               ...        ...        ...        ...        ...       ...   \n",
       "2023-06-16  2555.6001  2581.0000  2555.6001  2575.3999  2575.3999    150565   \n",
       "2023-06-19  2582.2000  2584.0000  2543.3999  2551.5000  2551.5000    165566   \n",
       "2023-06-20  2550.5000  2564.0000  2535.0000  2546.7500  2546.7500    171359   \n",
       "2023-06-21  2556.3501  2569.1001  2547.9500  2563.6001  2563.6001    144710   \n",
       "2023-06-22  2561.7000  2572.8999  2532.2500  2535.6001  2535.6001     61620   \n",
       "\n",
       "            DIVIDEND  SPLIT  \n",
       "2005-01-03       0.0    1.0  \n",
       "2005-01-04       0.0    1.0  \n",
       "2005-01-05       0.0    1.0  \n",
       "2005-01-06       0.0    1.0  \n",
       "2005-01-07       0.0    1.0  \n",
       "...              ...    ...  \n",
       "2023-06-16       0.0    1.0  \n",
       "2023-06-19       0.0    1.0  \n",
       "2023-06-20       0.0    1.0  \n",
       "2023-06-21       0.0    1.0  \n",
       "2023-06-22       0.0    1.0  \n",
       "\n",
       "[4553 rows x 8 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sort_index(ascending=True,inplace=True)\n",
    "df"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Another Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm (LSTM)                 (None, 100, 50)           10400     \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 100, 50)           0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 100, 50)           20200     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 100, 50)           0         \n",
      "                                                                 \n",
      " lstm_2 (LSTM)               (None, 50)                20200     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 50)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 51        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 50,851\n",
      "Trainable params: 50,851\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/25\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0084WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "140/140 [==============================] - 21s 89ms/step - loss: 0.0084\n",
      "Epoch 2/25\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0022WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "140/140 [==============================] - 11s 79ms/step - loss: 0.0022\n",
      "Epoch 3/25\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0021WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "140/140 [==============================] - 11s 80ms/step - loss: 0.0021\n",
      "Epoch 4/25\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0018WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "140/140 [==============================] - 11s 79ms/step - loss: 0.0018\n",
      "Epoch 5/25\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0016WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "140/140 [==============================] - 11s 79ms/step - loss: 0.0016\n",
      "Epoch 6/25\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0015WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "140/140 [==============================] - 11s 81ms/step - loss: 0.0015\n",
      "Epoch 7/25\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0014WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "140/140 [==============================] - 11s 79ms/step - loss: 0.0014\n",
      "Epoch 8/25\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0014WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "140/140 [==============================] - 11s 80ms/step - loss: 0.0014\n",
      "Epoch 9/25\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0013WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "140/140 [==============================] - 11s 81ms/step - loss: 0.0013\n",
      "Epoch 10/25\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0012WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "140/140 [==============================] - 11s 79ms/step - loss: 0.0012\n",
      "Epoch 11/25\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0010WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "140/140 [==============================] - 11s 81ms/step - loss: 0.0010\n",
      "Epoch 12/25\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0011WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "140/140 [==============================] - 12s 86ms/step - loss: 0.0011\n",
      "Epoch 13/25\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0010  WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "140/140 [==============================] - 11s 81ms/step - loss: 0.0010\n",
      "Epoch 14/25\n",
      "140/140 [==============================] - ETA: 0s - loss: 9.4201e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "140/140 [==============================] - 11s 78ms/step - loss: 9.4201e-04\n",
      "Epoch 15/25\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0010WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "140/140 [==============================] - 11s 79ms/step - loss: 0.0010\n",
      "Epoch 16/25\n",
      "140/140 [==============================] - ETA: 0s - loss: 9.4746e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "140/140 [==============================] - 11s 79ms/step - loss: 9.4746e-04\n",
      "Epoch 17/25\n",
      "140/140 [==============================] - ETA: 0s - loss: 9.9289e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "140/140 [==============================] - 11s 80ms/step - loss: 9.9289e-04\n",
      "Epoch 18/25\n",
      "140/140 [==============================] - ETA: 0s - loss: 9.4315e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "140/140 [==============================] - 12s 85ms/step - loss: 9.4315e-04\n",
      "Epoch 19/25\n",
      "140/140 [==============================] - ETA: 0s - loss: 0.0010WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "140/140 [==============================] - 11s 81ms/step - loss: 0.0010\n",
      "Epoch 20/25\n",
      "140/140 [==============================] - ETA: 0s - loss: 8.6787e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "140/140 [==============================] - 13s 95ms/step - loss: 8.6787e-04\n",
      "Epoch 21/25\n",
      "140/140 [==============================] - ETA: 0s - loss: 8.0082e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "140/140 [==============================] - 13s 95ms/step - loss: 8.0082e-04\n",
      "Epoch 22/25\n",
      "140/140 [==============================] - ETA: 0s - loss: 7.8880e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "140/140 [==============================] - 13s 94ms/step - loss: 7.8880e-04\n",
      "Epoch 23/25\n",
      "140/140 [==============================] - ETA: 0s - loss: 8.0365e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "140/140 [==============================] - 14s 98ms/step - loss: 8.0365e-04\n",
      "Epoch 24/25\n",
      "140/140 [==============================] - ETA: 0s - loss: 7.3225e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "140/140 [==============================] - 14s 101ms/step - loss: 7.3225e-04\n",
      "Epoch 25/25\n",
      "140/140 [==============================] - ETA: 0s - loss: 7.9254e-04WARNING:tensorflow:Can save best model only with val_loss available, skipping.\n",
      "140/140 [==============================] - 14s 100ms/step - loss: 7.9254e-04\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a2097e6940>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import os\n",
    "import datetime as dt\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, LSTM\n",
    "from keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "data = df\n",
    "\n",
    "scaler = MinMaxScaler(feature_range=(0,1))\n",
    "scaled_data = scaler.fit_transform(data['CLOSE'].values.reshape(-1,1))\n",
    "# Set the number of days used for prediction\n",
    "prediction_days = 100\n",
    "\n",
    "# Initialize empty lists for training data input and output\n",
    "x_train = []\n",
    "y_train = []\n",
    "\n",
    "# Iterate through the scaled data, starting from the prediction_days index\n",
    "for x in range(prediction_days, len(scaled_data)):\n",
    "    # Append the previous 'prediction_days' values to x_train\n",
    "    x_train.append(scaled_data[x - prediction_days:x, 0])\n",
    "    # Append the current value to y_train\n",
    "    y_train.append(scaled_data[x, 0])\n",
    "\n",
    "# Convert the x_train and y_train lists to numpy arrays\n",
    "x_train, y_train = np.array(x_train), np.array(y_train)\n",
    "\n",
    "# Reshape x_train to a 3D array with the appropriate dimensions for the LSTM model\n",
    "x_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n",
    "\n",
    "def LSTM_model():\n",
    "    \"\"\"\n",
    "    Create and configure an LSTM model for stock price prediction.\n",
    "\n",
    "    :return: The configured LSTM model (keras.Sequential)\n",
    "    \"\"\"\n",
    "\n",
    "    # Initialize a sequential model\n",
    "    model = Sequential()\n",
    "\n",
    "    # Add the first LSTM layer with 50 units, input shape, and return sequences\n",
    "    model.add(LSTM(units=50, return_sequences=True, input_shape=(x_train.shape[1], 1)))\n",
    "    # Add dropout to prevent overfitting\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Add a second LSTM layer with 50 units and return sequences\n",
    "    model.add(LSTM(units=50, return_sequences=True))\n",
    "    # Add dropout to prevent overfitting\n",
    "    model.add(Dropout(0.2))\n",
    "\n",
    "    # Add a third LSTM layer with 50 units\n",
    "    model.add(LSTM(units=50))\n",
    "    # Add dropout to prevent overfitting\n",
    "    model.add(Dropout(0.1))\n",
    "\n",
    "    # Add a dense output layer with one unit\n",
    "    model.add(Dense(units=1))\n",
    "\n",
    "    return model\n",
    "\n",
    "model = LSTM_model()\n",
    "model.summary()\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='mean_squared_error'\n",
    ")\n",
    "checkpointer = ModelCheckpoint(\n",
    "    filepath = 'weights_best.hdf5', \n",
    "    verbose = 2, \n",
    "    save_best_only = True\n",
    ")\n",
    "\n",
    "model.fit(\n",
    "    x_train, \n",
    "    y_train, \n",
    "    epochs=25, \n",
    "    batch_size = 32,\n",
    "    callbacks = [checkpointer]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143/143 [==============================] - 5s 34ms/step\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib' has no attribute 'get_data_path'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_9192\\2169673609.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     29\u001b[0m \u001b[0mpredicted_prices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     30\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 31\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     32\u001b[0m \u001b[1;31m# Invert the scaling applied to the predicted prices to obtain actual values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[0mpredicted_prices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mscaler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minverse_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_prices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda\\envs\\tf20\\lib\\site-packages\\matplotlib\\__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m    976\u001b[0m \u001b[1;31m# triggering resolution of _auto_backend_sentinel.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    977\u001b[0m rcParamsDefault = _rc_params_in_file(\n\u001b[1;32m--> 978\u001b[1;33m     \u001b[0mcbook\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_data_path\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"matplotlibrc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    979\u001b[0m     \u001b[1;31m# Strip leading comment.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    980\u001b[0m     \u001b[0mtransform\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"#\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mline\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32me:\\anaconda\\envs\\tf20\\lib\\site-packages\\matplotlib\\cbook\\__init__.py\u001b[0m in \u001b[0;36m_get_data_path\u001b[1;34m(*args)\u001b[0m\n\u001b[0;32m    557\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    558\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 559\u001b[1;33m \u001b[1;32mdef\u001b[0m \u001b[0mflatten\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mseq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscalarp\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_scalar_or_string\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    560\u001b[0m     \"\"\"\n\u001b[0;32m    561\u001b[0m     \u001b[0mReturn\u001b[0m \u001b[0ma\u001b[0m \u001b[0mgenerator\u001b[0m \u001b[0mof\u001b[0m \u001b[0mflattened\u001b[0m \u001b[0mnested\u001b[0m \u001b[0mcontainers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'matplotlib' has no attribute 'get_data_path'"
     ]
    }
   ],
   "source": [
    "test_data=data\n",
    "actual_prices = test_data['CLOSE'].values\n",
    "\n",
    "# Concatenate the training and test data along the 'Close' column\n",
    "total_dataset = pd.concat((data['CLOSE'], test_data['CLOSE']), axis=0)\n",
    "\n",
    "# Extract the relevant portion of the dataset for model inputs\n",
    "model_inputs = total_dataset[len(total_dataset) - len(test_data) - prediction_days:].values\n",
    "\n",
    "# Reshape the model inputs to a 2D array with a single column\n",
    "model_inputs = model_inputs.reshape(-1, 1)\n",
    "\n",
    "# Apply the same scaling used for training data to the model inputs\n",
    "model_inputs = scaler.transform(model_inputs)\n",
    "x_test = []\n",
    "\n",
    "# Iterate through the model inputs, starting from the prediction_days index\n",
    "for x in range(prediction_days, len(model_inputs)):\n",
    "    # Append the previous 'prediction_days' values to x_test\n",
    "    x_test.append(model_inputs[x-prediction_days:x, 0])\n",
    "\n",
    "# Convert the x_test list to a numpy array\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "# Reshape x_test to a 3D array with the appropriate dimensions for the LSTM model\n",
    "x_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1))\n",
    "\n",
    "# Generate price predictions using the LSTM model\n",
    "predicted_prices = model.predict(x_test)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "# Invert the scaling applied to the predicted prices to obtain actual values\n",
    "predicted_prices = scaler.inverse_transform(predicted_prices)\n",
    "plt.plot(actual_prices, color='black', label=f\"Actual price\")\n",
    "\n",
    "# Plot the predicted prices using a green linez\n",
    "plt.plot(predicted_prices, color='green', label=f\"Predicted price\")\n",
    "\n",
    "# Set the title of the plot using the company name\n",
    "plt.title(stock)\n",
    "\n",
    "# Set the x-axis label as 'time'\n",
    "plt.xlabel('time')\n",
    "\n",
    "# Set the y-axis label using the company name\n",
    "plt.ylabel(f\"share price\")\n",
    "\n",
    "# Display a legend to differentiate the actual and predicted prices\n",
    "plt.legend()\n",
    "\n",
    "# Show the plot on the screen\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 975ms/step\n",
      "Prediction: 2425.963623046875\n"
     ]
    }
   ],
   "source": [
    "real_data = [model_inputs[len(model_inputs) + 1 - prediction_days:len(model_inputs + 1), 0]]\n",
    "\n",
    "# Convert the real_data list to a numpy array\n",
    "real_data = np.array(real_data)\n",
    "\n",
    "# Reshape real_data to a 3D array with the appropriate dimensions for the LSTM model\n",
    "real_data = np.reshape(real_data, (real_data.shape[0], real_data.shape[1], 1))\n",
    "\n",
    "# Generate a prediction using the LSTM model with the real_data input\n",
    "prediction = model.predict(real_data)\n",
    "\n",
    "# Invert the scaling applied to the prediction to obtain the actual value\n",
    "prediction = scaler.inverse_transform(prediction)\n",
    "\n",
    "# Print the prediction result to the console\n",
    "print(f\"Prediction: {prediction[0][0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf20",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
